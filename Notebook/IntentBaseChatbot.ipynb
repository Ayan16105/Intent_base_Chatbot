{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Project Using Intents-Based Classification\n",
    "\n",
    "## 1. Introduction\n",
    " In this notebook, we will build a simple intents-based chatbot that utilizes Natural Language Processing (NLP) techniques\n",
    " to understand user inputs and provide contextually relevant responses. We'll use machine learning models for intent classification\n",
    " and Streamlit for the user interface.\n",
    " Let's start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import random\n",
    "import ssl\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import streamlit as st\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Download Necessary NLTK Resources\n",
    " Before we begin, let's download the necessary NLTK resources for tokenization, stopword removal, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Intents\n",
    " For this chatbot, we'll define a list of intents. Each intent will have:\n",
    " - A `tag` representing the intent category (e.g., \"greeting\")\n",
    " - `patterns` containing common user inputs related to that intent\n",
    " - `responses` with the chatbot's possible replies for that intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intents = [\n",
    "    {\"tag\": \"greeting\", \"patterns\": [\"Hi\", \"Hello\", \"How are you?\", \"Hey\"], \"responses\": [\"Hello!\", \"Hi there!\", \"How can I help you?\"]},\n",
    "    {\"tag\": \"goodbye\", \"patterns\": [\"Bye\", \"Goodbye\", \"See you later\"], \"responses\": [\"Goodbye!\", \"See you!\", \"Take care!\"]},\n",
    "    {\"tag\": \"help\", \"patterns\": [\"I need help\", \"Can you assist me?\", \"I don't understand\"], \"responses\": [\"How can I assist you?\", \"I'm here to help!\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Preprocessing the Text\n",
    " We will define a preprocessing function to clean and tokenize user inputs.\n",
    " This will involve:\n",
    " - Converting text to lowercase\n",
    " - Tokenizing the text into words\n",
    " - Removing stopwords (common words like \"the\", \"and\") and punctuation\n",
    " - Lemmatizing the words (reducing words to their root form)\n",
    "\n",
    " Define stopwords and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def regex_tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize the text by words\n",
    "\n",
    "def data_preprocess(text):\n",
    "    tokens = regex_tokenize(text)  # Tokenize the text\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word)  # Lemmatize each word\n",
    "        for word in tokens\n",
    "        if word not in string.punctuation and word not in stopwords_set  # Remove punctuation and stopwords\n",
    "    ]\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Model Training\n",
    " Next, we will prepare the data for training a machine learning model. We will use the `TfidfVectorizer` to convert the user input patterns\n",
    " into numerical features, and then train a `MultinomialNB` classifier to predict the intent.\n",
    "\n",
    " Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "labels = []\n",
    "for intent in intents:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)\n",
    "        labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Convert patterns into numerical data using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Save Model and Vectorizer\n",
    " It's good practice to save the trained model and vectorizer so we can use them later without retraining. We will save them using `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatbot_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('chatbot_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Chatbot Function\n",
    " Now we define the chatbot function that will take user input, preprocess it, and predict the intent using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(user_inp):\n",
    "    # Process the user input\n",
    "    user_inp = data_preprocess(user_inp)  # Preprocess user input\n",
    "    user_inp_vectorized = vectorizer.transform([' '.join(user_inp)])  # Vectorize the input\n",
    "    \n",
    "    # Predict the intent\n",
    "    tag = model.predict(user_inp_vectorized)[0]\n",
    "    \n",
    "    # Find the corresponding response\n",
    "    for intent in intents:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses'])\n",
    "    \n",
    "    return \"Sorry, I didn't understand that.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Streamlit Application\n",
    " To make this chatbot accessible through a web interface, we will use Streamlit to create an interactive UI.\n",
    " Streamlit allows us to build applications with minimal effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Intent-Based ChatBot\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []  # Initialize the session state to store chat messages\n",
    "\n",
    "# Display previous messages in the chat\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Get user input\n",
    "prompt = st.chat_input(\"Ask me here...\")\n",
    "if prompt:\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})  # Add user message to session state\n",
    "\n",
    "    # Get chatbot response\n",
    "    ans = chatbot(prompt)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(ans)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": ans})  # Add assistant response to session state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Future Improvements\n",
    " The current chatbot is quite basic. In the future, we can improve it by:\n",
    " - Expanding the dataset with more intents and patterns to make the chatbot more versatile.\n",
    " - Using advanced machine learning models, such as deep learning or BERT, to improve intent classification accuracy.\n",
    " - Adding the ability to handle multi-turn conversations for better user interactions.\n",
    " - Incorporating more complex natural language understanding features, such as named entity recognition (NER) and sentiment analysis.\n",
    "\n",
    " ## 10. Conclusion\n",
    " In this project, we successfully built an intents-based chatbot that uses NLP techniques to understand user input and generate contextually relevant responses.\n",
    " The chatbot is powered by a machine learning model (Naive Bayes) for intent classification and is deployed using Streamlit for an interactive user interface.\n",
    " With further improvements, this chatbot can be expanded to handle more complex interactions and be deployed in real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
